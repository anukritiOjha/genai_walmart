{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a6f23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain openai fastapi uvicorn sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c6e3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9583bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"APIKEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "289f3eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca8fa3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Tool (Example: get_weather)\n",
    "\n",
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Returns a mock weather report for the given city.\"\"\"\n",
    "    return f\"It's always sunny in {city} with 25°C!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08598c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize GPT Model (via LangChain)\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"gpt-4.1\", model_provider=\"openai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5887111",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the ReAct Agent (with Tool Binding)\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "tools = [get_weather]\n",
    "agent_executor = create_react_agent(model=model, tools=tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57764780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is the weather in Pune?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  get_weather (call_O6LVkzn1psF9tMVqnQcIwFhc)\n",
      " Call ID: call_O6LVkzn1psF9tMVqnQcIwFhc\n",
      "  Args:\n",
      "    city: Pune\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: get_weather\n",
      "\n",
      "It's always sunny in Pune with 25°C!\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The weather in Pune is sunny with a temperature of 25°C.\n"
     ]
    }
   ],
   "source": [
    "# Run the Agent Without Memory (Single Turn)\n",
    "\n",
    "input_message = {\"role\": \"user\", \"content\": \"What is the weather in Pune?\"}\n",
    "response = agent_executor.invoke({\"messages\": [input_message]})\n",
    "\n",
    "for msg in response[\"messages\"]:\n",
    "    msg.pretty_print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f67bcab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='My city is Nagpur', additional_kwargs={}, response_metadata={}, id='7f9af95d-d578-4afd-9604-7918be1ae277'),\n",
       "  AIMessage(content='Great! How can I assist you regarding your city, Nagpur? Would you like to know something specific—like the weather, tourist attractions, news, or anything else? Let me know how I can help!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 44, 'prompt_tokens': 52, 'total_tokens': 96, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': 'fp_51e1070cf2', 'id': 'chatcmpl-BlxLDEmcz8LcsR3LRfmVjbJqM9lSh', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--a47ad429-2426-4102-b211-5c8b42c6dd0b-0', usage_metadata={'input_tokens': 52, 'output_tokens': 44, 'total_tokens': 96, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  HumanMessage(content=\"What's the weather where I live?\", additional_kwargs={}, response_metadata={}, id='b5794b53-e7d5-4053-81c8-4f5897bcb575'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_51CejEJl4RZKlGYbcOZhtYAT', 'function': {'arguments': '{\"city\":\"Nagpur\"}', 'name': 'get_weather'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 110, 'total_tokens': 125, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': 'fp_51e1070cf2', 'id': 'chatcmpl-BlxLFB2ELOTr0mjyxqZgYZ1qPHA1t', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--5433d8bc-0828-451c-9e65-2555490f9f76-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'Nagpur'}, 'id': 'call_51CejEJl4RZKlGYbcOZhtYAT', 'type': 'tool_call'}], usage_metadata={'input_tokens': 110, 'output_tokens': 15, 'total_tokens': 125, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  ToolMessage(content=\"It's always sunny in Nagpur with 25°C!\", name='get_weather', id='4103872a-122e-40da-a4e9-98f201148535', tool_call_id='call_51CejEJl4RZKlGYbcOZhtYAT'),\n",
       "  AIMessage(content=\"Currently, it's always sunny in Nagpur with a temperature of 25°C! If you need more detailed weather information or forecasts, just let me know.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 32, 'prompt_tokens': 144, 'total_tokens': 176, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': 'fp_51e1070cf2', 'id': 'chatcmpl-BlxLGYAbc35U6X7xfWyTg5QqTYxkF', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--f08cd838-7277-4947-9b62-2d2d72f5f2e4-0', usage_metadata={'input_tokens': 144, 'output_tokens': 32, 'total_tokens': 176, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Add Memory for Multi-Turn Agent\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory = MemorySaver()\n",
    "agent_executor = create_react_agent(model=model, tools=tools, checkpointer=memory)\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"weather-001\"}}\n",
    "\n",
    "# Turn 1\n",
    "agent_executor.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"My city is Nagpur\"}]}, config)\n",
    "\n",
    "# Turn 2\n",
    "agent_executor.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"What's the weather where I live?\"}]}, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "395f91d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='My city is Nagpur', additional_kwargs={}, response_metadata={}, id='79353d6b-9235-4611-8359-69315213a816'),\n",
       "  AIMessage(content='Thanks for sharing! How can I assist you related to Nagpur? For example, would you like to know about the weather, places to visit, current news, or something else?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 38, 'prompt_tokens': 52, 'total_tokens': 90, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': 'fp_51e1070cf2', 'id': 'chatcmpl-BlxHop29k08qfwkUYPJVN2UIIbOsY', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--30bbe6a9-85cf-4fff-ad2c-d3925d4d8ec1-0', usage_metadata={'input_tokens': 52, 'output_tokens': 38, 'total_tokens': 90, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  HumanMessage(content=\"What's the weather where I live?\", additional_kwargs={}, response_metadata={}, id='445232d4-27bd-49e4-90c7-6eef195ec1f7'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_4hqX0OA0RvNzvLDkmxuEVPnB', 'function': {'arguments': '{\"city\":\"Nagpur\"}', 'name': 'get_weather'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 104, 'total_tokens': 119, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': 'fp_51e1070cf2', 'id': 'chatcmpl-BlxIFudFQn5XEEXGOLaAAXFoVJI0d', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--ff915bc7-dfe7-4ca8-8aaa-0db959c8878e-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'Nagpur'}, 'id': 'call_4hqX0OA0RvNzvLDkmxuEVPnB', 'type': 'tool_call'}], usage_metadata={'input_tokens': 104, 'output_tokens': 15, 'total_tokens': 119, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  ToolMessage(content=\"It's always sunny in Nagpur with 25°C!\", name='get_weather', id='9bfec8a3-ca12-4dc3-a407-409abeaa4ab5', tool_call_id='call_4hqX0OA0RvNzvLDkmxuEVPnB'),\n",
       "  AIMessage(content='The weather in Nagpur is currently sunny with a temperature of 25°C. If you need a more detailed forecast or want updates for other days, let me know!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 35, 'prompt_tokens': 138, 'total_tokens': 173, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': 'fp_51e1070cf2', 'id': 'chatcmpl-BlxIHPor1NRIBmWjmn73uoXPmhR7t', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--06a984a3-a3c9-4bdd-b2df-00fe092f0718-0', usage_metadata={'input_tokens': 138, 'output_tokens': 35, 'total_tokens': 173, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn 2\n",
    "agent_executor.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"What's the weather where I live?\"}]}, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71db85d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Let’s break down possible causes for **Walmart’s 20% sales increase this month**—step by step:\n",
      "\n",
      "---\n",
      "\n",
      "### 1. **Internal Factors**\n",
      "\n",
      "#### **a. Promotions & Discounts**\n",
      "- **Major sales events**: Did Walmart run large promotions (e.g., Memorial Day, early back-to-school) or flash sales?\n",
      "- **Discounts or price matches**: Lower prices can drive volume.\n",
      "\n",
      "#### **b. New Product Launches**\n",
      "- Did Walmart introduce highly anticipated products or exclusive merchandise?\n",
      "- Extended private-label offerings (like Great Value) might boost sales.\n",
      "\n",
      "#### **c. Expansion**\n",
      "- Opened new stores or revamped existing ones.\n",
      "- Enhanced online delivery to more locations.\n",
      "\n",
      "#### **d. E-commerce Growth**\n",
      "- Improved app/website usability, new delivery/pickup options, or digital marketing.\n",
      "- Expanded grocery pickup/delivery may have drawn more online orders.\n",
      "\n",
      "#### **e. Loyalty Programs**\n",
      "- Reward or membership programs incentivized spending.\n",
      "\n",
      "---\n",
      "\n",
      "### 2. **External Factors**\n",
      "\n",
      "#### **a. Macroeconomic Shifts**\n",
      "- **Stimulus or tax returns:** Customers may have had extra disposable income.\n",
      "- **Inflation:** Higher prices (even if sales volume is steady) result in higher dollar sales.\n",
      "\n",
      "#### **b. Competitor Weakness**\n",
      "- **Temporary closure** or challenges at competitors (e.g., Target, local grocers).\n",
      "- Competitors had stockouts while Walmart remained well-stocked.\n",
      "\n",
      "#### **c. Seasonality**\n",
      "- Time of year: Some months see organic lifts (e.g., start of summer, holidays).\n",
      "\n",
      "#### **d. Socio-Environmental Events**\n",
      "- **Weather events:** Hurricanes, floods, or storms may drive emergency purchases.\n",
      "- **Public health scares** (e.g., new COVID-19 variant) could boost grocery/sanitary sales.\n",
      "\n",
      "---\n",
      "\n",
      "### 3. **Other Possible Reasons**\n",
      "\n",
      "#### **a. Calculation Methods**\n",
      "- Change in fiscal calendar or reported weeks.\n",
      "- Comparison to unusually weak sales last year (makes this year’s growth appear large).\n",
      "\n",
      "#### **b. Acquisitions**\n",
      "- Walmart acquired or merged with another retailer, adding their sales figures.\n",
      "\n",
      "---\n",
      "\n",
      "### **Summary Table**\n",
      "\n",
      "| Category              | Examples                                                             |\n",
      "|-----------------------|---------------------------------------------------------------------|\n",
      "| Promotions/exclusives | Major discounts, new products, loyalty offers                       |\n",
      "| E-commerce            | More online orders, new options, tech investments                   |\n",
      "| Economic Forces       | Stimulus checks, higher prices (inflation), consumer confidence     |\n",
      "| Competitive Shifts    | Competitor weaknesses, stockouts elsewhere                          |\n",
      "| Seasonality/events    | Holiday/summer spikes, emergencies                                  |\n",
      "\n",
      "---\n",
      "\n",
      "**Conclusion:**  \n",
      "A 20% sales jump is unusually high for Walmart, so it’s likely due to a combination of factors above. The most probable drivers: major promotions, increased e-commerce activity, price inflation, or external shocks (stimulus, disaster, or competitor disruption). For specifics, reviewing Walmart’s own press releases or financial reports for the month often gives precise reasons.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"gpt-4.1\", model_provider=\"openai\")\n",
    "\n",
    "query = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Walmart's sales increased by 20% this month. Think step-by-step about possible causes.\"\n",
    "}\n",
    "\n",
    "response = model.invoke([query])\n",
    "print(response.text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067c5bad",
   "metadata": {},
   "source": [
    "What is Plan-and-Execute (PnE)?\n",
    "\n",
    "Plan-and-Execute is a powerful agent pattern that:\n",
    "•\tFirst plans a sequence of subtasks based on the user request.\n",
    "•\tThen executes each step, often using tools, APIs, or functions.\n",
    "This improves reliability, transparency, and modularity in AI task solving."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14f338e",
   "metadata": {},
   "source": [
    "Use Case:\n",
    "Prompt: “Launch a weekend sale campaign across 3 top-selling categories.”\n",
    " \n",
    "Step 1: Plan\n",
    "The agent first thinks through all the required steps before doing anything:\n",
    "\n",
    "PLAN:\n",
    "To launch a weekend sale campaign across the top 3 categories, the following steps are required:\n",
    "\n",
    "1. Identify the top 3 selling product categories for the last 30 days.\n",
    "2. Retrieve current pricing and inventory levels for these categories.\n",
    "3. Draft a weekend sale campaign with discounts and promo copy.\n",
    "4. Submit the campaign for manager approval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeaafd16",
   "metadata": {},
   "source": [
    "Step 2: Execute Each Step\n",
    "Each planned step is then executed one by one, using appropriate tools or API functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98d8111",
   "metadata": {},
   "source": [
    "✅ Step 1: Identify top categories\n",
    "🛠️ Tool Call → get_top_categories(last_n_days=30, top_k=3)\n",
    "\n",
    "Response:\n",
    "[\"Groceries\", \"Electronics\", \"Personal Care\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5b2c8c",
   "metadata": {},
   "source": [
    "✅ Step 2: Fetch pricing and stock\n",
    "🛠️ Tool Call → get_pricing_and_inventory([\"Groceries\", \"Electronics\", \"Personal Care\"])\n",
    "Response:\n",
    "{\n",
    "  \"Groceries\": {\"avg_price\": \"$3.5\", \"in_stock\": \"High\"},\n",
    "  \"Electronics\": {\"avg_price\": \"$120\", \"in_stock\": \"Moderate\"},\n",
    "  \"Personal Care\": {\"avg_price\": \"$8\", \"in_stock\": \"High\"}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd26da4d",
   "metadata": {},
   "source": [
    "✅ Step 3: Create campaign draft\n",
    "🛠️ Tool Call → create_campaign_draft(...)\n",
    "Response:\n",
    "Drafted a weekend promo:  \n",
    "🛒 Groceries – 15% off  \n",
    "🔌 Electronics – 10% cashback  \n",
    "🧼 Personal Care – Buy 1 Get 1 Free  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651bf786",
   "metadata": {},
   "source": [
    "✅ Step 4: Submit for approval\n",
    "🛠️ Tool Call → submit_campaign(draft_id=\"promo_2025_weekend_001\")\n",
    "Response:\n",
    "✅ Campaign submitted for approval. Awaiting response from Marketing Lead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7103b7d",
   "metadata": {},
   "source": [
    "✅ Final Response to User:\n",
    "🎯 Campaign Planning & Execution Complete!\n",
    "\n",
    "🔹 Categories Selected: Groceries, Electronics, Personal Care  \n",
    "🔹 Campaign Draft: Discounts + Promo Copy Created  \n",
    "🔹 Status: Submitted for approval (ID: promo_2025_weekend_001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c713df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting weekend_sale_agent.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile weekend_sale_agent.py\n",
    "#In Code (LangChain-style)\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.tools import tool\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "\n",
    "# Define each tool step\n",
    "@tool\n",
    "def get_top_categories(last_n_days: int, top_k: int):\n",
    "    \"\"\"Return top-k selling categories in the last N days.\"\"\"\n",
    "    return [\"Groceries\", \"Electronics\", \"Personal Care\"]\n",
    "\n",
    "@tool\n",
    "def get_pricing_and_inventory(categories: list):\n",
    "    \"\"\"Return mock pricing and inventory details for given categories.\"\"\"\n",
    "    return {\n",
    "        \"Groceries\": {\"avg_price\": \"$3.5\", \"in_stock\": \"High\"},\n",
    "        \"Electronics\": {\"avg_price\": \"$120\", \"in_stock\": \"Moderate\"},\n",
    "        \"Personal Care\": {\"avg_price\": \"$8\", \"in_stock\": \"High\"},\n",
    "    }\n",
    "\n",
    "@tool\n",
    "def create_campaign_draft(data: dict):\n",
    "    \"\"\"Create a weekend sales campaign draft for the categories.\"\"\"\n",
    "    return \"Drafted weekend sale promo with discounts.\"\n",
    "\n",
    "@tool\n",
    "def submit_campaign(draft_id: str):\n",
    "    \"\"\"Submit the campaign draft for approval.\"\"\"\n",
    "    return \"Campaign submitted for approval.\"\n",
    "\n",
    "# Combine tools\n",
    "tools = [get_top_categories, get_pricing_and_inventory, create_campaign_draft, submit_campaign]\n",
    "\n",
    "# Initialize agent\n",
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0.3)\n",
    "agent = initialize_agent(tools, llm, agent_type=\"chat-zero-shot-react-description\")\n",
    "\n",
    "# Run\n",
    "agent.run(\"Launch a weekend sale campaign across 3 top-selling categories.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff6bd7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing weekend_sale_agent1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile weekend_sale_agent1.py\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# ✅ Define your tools with docstrings\n",
    "@tool\n",
    "def get_top_categories() -> list:\n",
    "    \"\"\"Returns top 3 selling product categories.\"\"\"\n",
    "    return [\"Groceries\", \"Electronics\", \"Personal Care\"]\n",
    "\n",
    "@tool\n",
    "def get_pricing_and_inventory() -> dict:\n",
    "    \"\"\"Returns pricing and inventory levels.\"\"\"\n",
    "    return {\n",
    "        \"Groceries\": {\"avg_price\": \"$3.5\", \"in_stock\": \"High\"},\n",
    "        \"Electronics\": {\"avg_price\": \"$120\", \"in_stock\": \"Moderate\"},\n",
    "        \"Personal Care\": {\"avg_price\": \"$8\", \"in_stock\": \"High\"},\n",
    "    }\n",
    "\n",
    "@tool\n",
    "def create_campaign_draft() -> str:\n",
    "    \"\"\"Creates a campaign draft with promotions.\"\"\"\n",
    "    return \"\"\"\n",
    "    🛒 Groceries – 15% off\n",
    "    🔌 Electronics – 10% cashback\n",
    "    🧼 Personal Care – Buy 1 Get 1 Free\n",
    "    \"\"\"\n",
    "\n",
    "@tool\n",
    "def submit_campaign() -> str:\n",
    "    \"\"\"Submits the campaign for approval.\"\"\"\n",
    "    return \"✅ Campaign submitted for approval. ID: promo_2025_weekend_001\"\n",
    "\n",
    "# ✅ Load GPT-4 model (or GPT-3.5 for free tier)\n",
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0.3)\n",
    "\n",
    "# ✅ Register tools\n",
    "tools = [get_top_categories, get_pricing_and_inventory, create_campaign_draft, submit_campaign]\n",
    "\n",
    "# ✅ Create agent using LangGraph\n",
    "memory = MemorySaver()\n",
    "agent_executor = create_react_agent(llm, tools, checkpointer=memory)\n",
    "\n",
    "# ✅ Set thread ID\n",
    "config = {\"configurable\": {\"thread_id\": \"campaign-001\"}}\n",
    "\n",
    "# ✅ Ask agent to plan and execute\n",
    "input_msg = {\"role\": \"user\", \"content\": \"Launch a weekend sale campaign across 3 top-selling categories.\"}\n",
    "\n",
    "for step in agent_executor.stream({\"messages\": [input_msg]}, config, stream_mode=\"values\"):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aef93d4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f299c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting langchain_agent.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile langchain_agent.py\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import TypedDict, List, Union\n",
    "import os\n",
    "\n",
    "OPEN_API_KEY = \" APIKEY\"  # Replace with your OpenAI API key\n",
    "\n",
    "# 🔐 Set your OpenAI key\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPEN_API_KEY\n",
    "# 1. Define tool\n",
    "@tool\n",
    "def calculator(expression: str) -> str:\n",
    "    \"\"\"Evaluates a basic arithmetic expression.\"\"\"\n",
    "    return str(eval(expression))\n",
    "\n",
    "# 2. Define state schema\n",
    "class AgentState(TypedDict):\n",
    "    messages: List[HumanMessage]\n",
    "\n",
    "# 3. Define LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# 4. Wrap LLM with a lambda that extracts `messages` from the state\n",
    "llm_node = RunnableLambda(lambda state: llm.invoke(state[\"messages\"]))\n",
    "\n",
    "# 5. Wrap tool with dummy logic (you can later route to actual tool parsing)\n",
    "# tool_node = RunnableLambda(lambda state: {\"messages\": [HumanMessage(content=calculator(\"(45 * 23) + 129\"))]})\n",
    "# Tool node that evaluates the user's expression dynamically\n",
    "tool_node = RunnableLambda(\n",
    "    lambda state: {\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=calculator(state[\"messages\"][-1].content))\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "# 6. Build graph\n",
    "graph_builder = StateGraph(AgentState)\n",
    "graph_builder.add_node(\"llm\", llm_node)\n",
    "graph_builder.add_node(\"calc_tool\", tool_node)\n",
    "\n",
    "graph_builder.set_entry_point(\"llm\")\n",
    "graph_builder.add_edge(\"llm\", \"calc_tool\")\n",
    "graph_builder.add_edge(\"calc_tool\", END)\n",
    "\n",
    "# 7. Compile\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "# 8. Invoke the agent with a message\n",
    "result = graph.invoke({\"messages\": [ HumanMessage(content=\"(99 + 1) * 2\")]})\n",
    "#print(\"🤖 Agent Final Response:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "645ec1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing langchain_agent1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile langchain_agent1.py\n",
    "\n",
    "from langgraph.graph import StateGraph\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from typing import TypedDict\n",
    "from langchain.agents import Tool\n",
    "from langchain.agents.agent_toolkits import create_python_agent\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain.agents.initialize import initialize_agent\n",
    "from langchain.agents.agent_toolkits.python.base import PythonREPLTool\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Step 1: Define state schema (input is a string)\n",
    "class AgentState(TypedDict):\n",
    "    input: str\n",
    "    output: str\n",
    "\n",
    "# Step 2: LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# Step 3: Tool (Python calculator)\n",
    "calculator = PythonREPLTool()\n",
    "\n",
    "# Step 4: Tool execution node (simple Runnable)\n",
    "def run_calculator(state: AgentState) -> AgentState:\n",
    "    expression = state[\"input\"]\n",
    "    result = calculator.invoke(expression)\n",
    "    return {\"input\": expression, \"output\": result}\n",
    "\n",
    "tool_node = RunnableLambda(run_calculator)\n",
    "\n",
    "# Step 5: Build LangGraph\n",
    "builder = StateGraph(AgentState)\n",
    "builder.add_node(\"calculator\", tool_node)\n",
    "builder.set_entry_point(\"calculator\")\n",
    "builder.set_finish_point(\"calculator\")\n",
    "graph = builder.compile()\n",
    "\n",
    "# Step 6: Invoke the agent\n",
    "if __name__ == \"__main__\":\n",
    "    query = \"(99 + 1) * 2\"\n",
    "    result = graph.invoke({\"input\": query})\n",
    "    print(f\"🤖 Final Answer for '{query}': {result['output']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b986fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting walmart_mcp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile walmart_mcp.py\n",
    "\n",
    "# Walmart MCP Agent using LangChain and FastAPI\n",
    "# This agent converts natural language queries into SQL queries using OpenAI's GPT-4o model\n",
    "# and maintains session memory for context.\n",
    "\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from fastapi.responses import JSONResponse\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "import os, traceback\n",
    "from pprint import pprint\n",
    "\n",
    "OPEN_API_KEY = \"API KEY\"  # Replace with your OpenAI API key\n",
    "# 🔐 Set API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPEN_API_KEY  # Use GPT-4o\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# LLM Model\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-4o\")\n",
    "\n",
    "# Request body\n",
    "class QueryRequest(BaseModel):\n",
    "    session_id: str\n",
    "    query: str\n",
    "\n",
    "# Session memory\n",
    "session_store = {}\n",
    "\n",
    "def get_memory(session_id: str):\n",
    "    if session_id not in session_store:\n",
    "        session_store[session_id] = ChatMessageHistory()\n",
    "    return session_store[session_id]\n",
    "\n",
    "# Natural language → SQL template\n",
    "def query_to_sql(messages):\n",
    "    messages = messages[\"input\"]\n",
    "    user_input = messages[-1].content\n",
    "    llm_messages = [\n",
    "        HumanMessage(content=f\"Convert the following natural language query to SQL: {user_input}\")\n",
    "    ]\n",
    "    pprint(llm_messages)\n",
    "    return llm.invoke(llm_messages) # Invoke the LLM to get SQL query\n",
    "\n",
    "#Wrap with LangChain memory handler\n",
    "chain = RunnableWithMessageHistory(\n",
    "    RunnableLambda(query_to_sql),\n",
    "    lambda session_id: get_memory(session_id),\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\",\n",
    ")\n",
    "\n",
    "# 🚀 Endpoint to handle Walmart agent queries\n",
    "@app.post(\"/mcp\")\n",
    "async def handle_query(request: QueryRequest):\n",
    "    try:\n",
    "        session_id = request.session_id\n",
    "        query = request.query\n",
    "\n",
    "        print(f\"[Walmart-MCP] Session: {session_id} | Query: {query}\")\n",
    "\n",
    "        response = await chain.ainvoke(\n",
    "            {\"input\": [HumanMessage(content=query)]},  #\n",
    "            config={\"configurable\": {\"session_id\": session_id}},\n",
    "        )\n",
    "        return {\"response\": response.content}\n",
    "\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        return JSONResponse(status_code=500, content={\"error\": str(e)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e47e0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting walmart_mcp_schema.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile walmart_mcp_schema.py\n",
    "\n",
    "# Walmart MCP Agent with Schema-aware SQL Generation\n",
    "# This agent uses LangChain to convert natural language queries into SQL queries\n",
    "# while being aware of the database schema, enhancing accuracy and relevance.\n",
    "# It maintains session memory for context and uses FastAPI for the web interface.\n",
    "# Requirements: langchain, openai, fastapi, uvicorn, sqlite3\n",
    "# LangChain SQL Generator Agent (Schema-aware)\n",
    "\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from fastapi.responses import JSONResponse\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "import os, traceback\n",
    "\n",
    "\n",
    "OPEN_API_KEY = \"API KEY\"\n",
    "# 1. Set API Key\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPEN_API_KEY  # Use GPT-4o\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# 2. Language Model\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# 3. Schema definition (for prompt context)\n",
    "schema_context = \"\"\"\n",
    "Table: products\n",
    "Columns: product_id, product_name, category, price, location, sales_volume\n",
    "\n",
    "Table: customers\n",
    "Columns: customer_id, name, city, age, gender\n",
    "\"\"\"\n",
    "\n",
    "# 4. Prompt Template (Prompt Engineering for SQL)\n",
    "sql_prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "You are an expert data analyst. Convert the user's natural language query into a SQL query.\n",
    "Use the following table schema:\n",
    "{schema}\n",
    "\n",
    "Query: {user_query}\n",
    "SQL:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# 5. Request Body\n",
    "class QueryRequest(BaseModel):\n",
    "    session_id: str\n",
    "    query: str\n",
    "\n",
    "# 6. In-memory Session Store\n",
    "session_store = {}\n",
    "\n",
    "def get_memory(session_id: str):\n",
    "    if session_id not in session_store:\n",
    "        session_store[session_id] = ChatMessageHistory()\n",
    "    return session_store[session_id]\n",
    "\n",
    "# 7. Tool Execution Chain\n",
    "def generate_sql(messages):\n",
    "    user_message = messages[\"input\"][-1].content\n",
    "    final_prompt = sql_prompt_template.format(schema=schema_context, user_query=user_message)\n",
    "    return llm.invoke([HumanMessage(content=final_prompt)])\n",
    "\n",
    "chain = RunnableWithMessageHistory(\n",
    "    RunnableLambda(generate_sql),\n",
    "    lambda session_id: get_memory(session_id),\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\",\n",
    ")\n",
    "\n",
    "# 8. Endpoint\n",
    "@app.post(\"/generate-sql\")\n",
    "async def sql_agent(request: QueryRequest):\n",
    "    try:\n",
    "        session_id = request.session_id\n",
    "        query = request.query\n",
    "\n",
    "        response = await chain.ainvoke(\n",
    "            {\"input\": [HumanMessage(content=query)]},\n",
    "            config={\"configurable\": {\"session_id\": session_id}},\n",
    "        )\n",
    "\n",
    "        return {\"sql_query\": response.content}\n",
    "\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        return JSONResponse(status_code=500, content={\"error\": str(e)})\n",
    "\n",
    "\n",
    "from fastapi import Query\n",
    "\n",
    "@app.get(\"/mcp\")\n",
    "async def handle_query_get(session_id: str = Query(...), query: str = Query(...)):\n",
    "    try:\n",
    "        print(f\"[Walmart-MCP-GET] Session: {session_id} | Query: {query}\")\n",
    "\n",
    "        response = await chain.ainvoke(\n",
    "            {\"input\": [HumanMessage(content=query)]},\n",
    "            config={\"configurable\": {\"session_id\": session_id}},\n",
    "        )\n",
    "        return {\"response\": response.content}\n",
    "\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        return JSONResponse(status_code=500, content={\"error\": str(e)})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72dee1b",
   "metadata": {},
   "source": [
    "http://127.0.0.1:8000/mcp?session_id=user001&query=Show%20top%205%20selling%20items%20in%20Pune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "91ebfbd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain version: 0.3.26\n",
      "LangChain Core version: 0.3.66\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "import langchain_core\n",
    "print(\"LangChain version:\", langchain.__version__)\n",
    "print(\"LangChain Core version:\", langchain_core.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef48c78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_community.tools.python_repl.tool import PythonREPLTool\n",
    "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "# 1. Define the state\n",
    "def run_python_code(state):\n",
    "    tool = PythonREPLTool()\n",
    "    input_text = state[\"messages\"][-1].content\n",
    "    result = tool.invoke(input_text)\n",
    "    return {\"messages\": [HumanMessage(content=str(result))]}\n",
    "\n",
    "# 2. Convert tool to OpenAI-compatible function\n",
    "py_tool = convert_to_openai_tool(run_python_code)\n",
    "\n",
    "# 3. Create LLM and tool node\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "llm_node = ToolNode(llm)\n",
    "\n",
    "# 4. Create the graph\n",
    "builder = StateGraph(schema={\"messages\": list})\n",
    "\n",
    "builder.add_node(\"llm\", llm_node)\n",
    "builder.add_node(\"python\", RunnableLambda(run_python_code))\n",
    "\n",
    "# Conditional routing\n",
    "builder.set_conditional_entry_point(\"llm\", tools_condition)\n",
    "builder.add_edge(\"llm\", \"python\")\n",
    "builder.add_edge(\"python\", END)\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "# 5. Run the agent\n",
    "result = graph.invoke({\n",
    "    \"messages\": [HumanMessage(content=\"What is (45 * 23) + 129 ?\")]\n",
    "})\n",
    "\n",
    "# 6. Print final result\n",
    "final_message = result[\"messages\"][-1].content\n",
    "print(\"🤖 Agent Final Response:\", final_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c5bb932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting langgraph_python_repl.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile langgraph_python_repl.py\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_community.tools.python_repl import PythonREPLTool\n",
    "\n",
    "# Step 1: Define the shared state\n",
    "class AgentState(dict):\n",
    "    pass\n",
    "\n",
    "# Step 2: Define the REPL tool\n",
    "python_repl = PythonREPLTool()\n",
    "\n",
    "# Step 3: Define a LangGraph node that uses the REPL\n",
    "def run_python(state: AgentState) -> AgentState:\n",
    "    question = state[\"messages\"][-1].content\n",
    "    print(f\"💡 Executing: {question}\")\n",
    "    result = python_repl.invoke(question)\n",
    "    return {\"messages\": state[\"messages\"] + [AIMessage(content=result)]}\n",
    "\n",
    "tool_node = RunnableLambda(run_python)\n",
    "\n",
    "# Step 4: Build the LangGraph\n",
    "builder = StateGraph(AgentState)\n",
    "builder.add_node(\"python\", tool_node)\n",
    "builder.set_entry_point(\"python\")\n",
    "builder.set_finish_point(\"python\")\n",
    "graph = builder.compile()\n",
    "\n",
    "# Step 5: Run it\n",
    "result = graph.invoke({\"messages\": [HumanMessage(content=\"(45 * 23) + 129\")]})\n",
    "print(\"🤖 Agent Final Response:\", result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa015b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing langgraph_agent2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile langgraph_agent2.py\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.tools.python_repl import PythonREPLTool\n",
    "\n",
    "# Step 1: Define shared state\n",
    "class AgentState(dict):\n",
    "    pass\n",
    "\n",
    "# Step 2: Initialize tools\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-4o\")  # Or use Gemini if preferred\n",
    "python_tool = PythonREPLTool()\n",
    "\n",
    "# Step 3: Define logic nodes\n",
    "\n",
    "# 3a. Reasoning node using LLM\n",
    "def decide_action(state: AgentState) -> AgentState:\n",
    "    user_msg = state[\"messages\"][-1].content\n",
    "    tool_prompt = f\"\"\"You are a smart assistant. Decide:\n",
    "\n",
    "- If the question needs Python calculation, say: \"use_tool\"\n",
    "- Else answer directly\n",
    "\n",
    "Query: {user_msg}\n",
    "Answer:\"\"\"\n",
    "    decision = llm.invoke([HumanMessage(content=tool_prompt)])\n",
    "    print(f\"🤖 LLM Decision: {decision.content}\")\n",
    "    if \"use_tool\" in decision.content.lower():\n",
    "        return {\"messages\": state[\"messages\"], \"use_tool\": True}\n",
    "    return {\"messages\": state[\"messages\"] + [AIMessage(content=decision.content)], \"use_tool\": False}\n",
    "\n",
    "# 3b. Tool execution node\n",
    "def run_python(state: AgentState) -> AgentState:\n",
    "    user_input = state[\"messages\"][-1].content\n",
    "    output = python_tool.invoke(user_input)\n",
    "    return {\"messages\": state[\"messages\"] + [AIMessage(content=output)]}\n",
    "\n",
    "# Step 4: LangGraph setup\n",
    "builder = StateGraph(AgentState)\n",
    "builder.add_node(\"decide\", RunnableLambda(decide_action))\n",
    "builder.add_node(\"python\", RunnableLambda(run_python))\n",
    "\n",
    "# Conditional routing\n",
    "def should_use_tool(state: AgentState) -> str:\n",
    "    return \"python\" if state.get(\"use_tool\") else END\n",
    "\n",
    "builder.set_entry_point(\"decide\")\n",
    "builder.add_conditional_edges(\"decide\", should_use_tool, {\"python\": \"python\", END: END})\n",
    "builder.set_finish_point(\"python\")\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "# Step 5: Run agent\n",
    "result = graph.invoke({\"messages\": [HumanMessage(content=\"What is (45 * 23) + 129?\")]})\n",
    "print(\"🧠 Final Response:\", result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ffed71a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I’m ChatGPT, an AI language model created by OpenAI. I’m here to help answer your questions, have conversations, and assist with a wide range of topics. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "# Set your OpenAI API key securely if not already set in environment\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"🔐 Enter your OpenAI API key: \")\n",
    "\n",
    "# Import LangChain's chat model initializer\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# Initialize GPT-4.1 model from OpenAI\n",
    "model = init_chat_model(\n",
    "    model=\"gpt-4.1\",\n",
    "    model_provider=\"openai\",     # Explicitly tell LangChain to use OpenAI\n",
    "    temperature=0.0              # Optional: make outputs deterministic\n",
    ")\n",
    "\n",
    "# Test the model\n",
    "response = model.invoke([{\"role\": \"user\", \"content\": \"Hi there! Who are you?\"}])\n",
    "print(response.text())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
